%Matrix Calculus
%HU, Pili
%Craete: 20120330

%Modify the relative path accordingly
\input{../../common/common.tex}

%This usually doesn't need modification 
\author{HU, Pili\thanks{hupili [at] ie [dot] cuhk [dot] edu [dot] hk}}

%Modify them accordingly===
\title{Matrix Calculus: \\ Derivation and Simple Application}
\date{March 30, 2012\thanks{Last compile:\today}}

\begin{document}

\maketitle
%>============================================
\begin{abstract}
	Matrix Calculus\cite{wiki_mc} is a very useful tool in many 
	engineering problems. Basic rules of matrix calculus are 
	nothing more than ordinary calculus rules covered in 
	undergraduate courses. However, using matrix calculus, 
	the derivation process is more compact. This document is 
	adapted from the notes of a course the author recently attends.
	It builds matrix calculus from scratch. Only prerequisites 
	are basic calculus notions and linear algebra operation.  
	To get a quick executive guide, please refer to the cheat 
	sheet in the end. 
\end{abstract}
%<=======Abstract ENd=========================

%>============================================
\pagebreak
\tableofcontents
\pagebreak
%<=======TOC ENd==============================



\section{Introductory Example}

We start with an one variable linear function:
\begin{equation}
	f(x) = ax
\end{equation}

To be coherent, we abuse the partial derivative notation:
\begin{equation}
	\pdiff{f}{x} = a
	\label{eq:fax-single}
\end{equation}

Extending this function to be multivariate, we have:
\begin{equation}
	f(x) = \sum_{i}{a_ix_i} = \tran{a}x
\end{equation}
%The followings are the suggested transpose online? 
%What do you prefer? 
%I choose \mathrm at present. It looks reasonably good. 
%	a^\intercal
%	a^\mathsf{T}
%	a^\mathrm{T}
%	a^\top
%	a^\bot
Where $a = \tran{[a_1,a_2,\ldots,a_n]}$ and 
$x = \tran{[x_1,x_2,\ldots,x_n]}$. 
We first compute partial derivatives directly:
\begin{equation}
	\pdiff{f}{x_k} = \pdiff{(\sum_{i}{a_ix_i})}{x_k} = a_k 
\end{equation}
for all $k=1,2, \ldots, n$. Then we organize $n$ partial derivatives
in the following way:
\begin{equation}
	\pdiff{f}{x} = \left[
	\begin{matrix}
		\pdiff{f}{x_1} \\
		\pdiff{f}{x_2} \\
		\vdots \\
		\pdiff{f}{x_n}
	\end{matrix}
	\right]
	= \left[
	\begin{matrix}
		a_1 \\
		a_2 \\
		\vdots \\
		a_n
	\end{matrix}
	\right]
	= a
	\label{eq:fax-multi}
\end{equation}
The first equality is by proper definition and the rest roots from 
ordinary calculus rules. 

Eqn(\ref{eq:fax-multi}) is analogous to eqn(\ref{eq:fax-single}), except
the variable changes from a scalar to a vector. Thus we want to directly 
claim the result of eqn(\ref{eq:fax-multi}) without those intermediate steps 
solving for partial derivatives separately. Actually, we'll see soon 
that eqn(\ref{eq:fax-multi}) plays a core role in matrix calculus. 

Following sections are organized as follows:
\begin{itemize}
	\item Section(\ref{sec:derivation}) builds commonly used 
	matrix calculus rules from ordinary calculus and linear 
	algebra. Necessary and important properties of linear 
	algebra is also proved along the way. This section is not 
	organized afterhand. All results are proved when we need them. 
	\item Section(\ref{sec:application}) shows some applications 
	using matrix calculus. 
	\item Section(\ref{sec:cheat}) concludes a cheat sheet of 
	matrix calculus. Note that this cheat sheet may be different 
	from others. Users need to figure out some basic definitions 
	before applying the rules. 
\end{itemize}


\section{Derivation}
\label{sec:derivation}

\subsection{Organization of Elements}
From the introductary example, we already see that matrix calculus 
does not distinguish from ordinary calculus by fundamental rules. 
However, with better organization of elements and 
proving useful properties, we can simplify the derivation process 
in real problems. 

The author would like to adopt the following definition:
\begin{mydef}
	\label{def:org}
	For a scalar valued function $f(x)$, the result $\pdiff{f}{x}$
	has the same size with $x$. That is 
	\begin{equation}
		\pdiff{f}{x} = 
		\left[
		\begin{matrix}
			\pdiff{f}{x_{11}} & \pdiff{f}{x_{12}} & \ldots &\pdiff{f}{x_{1n}}\\
			\pdiff{f}{x_{21}} & \pdiff{f}{x_{22}} & \ldots &\pdiff{f}{x_{2n}}\\
			\vdots & \vdots & \ddots & \vdots \\
			\pdiff{f}{x_{m1}} & \pdiff{f}{x_{m2}} & \ldots &\pdiff{f}{x_{mn}}\\
		\end{matrix}
		\right]
	\end{equation}
\end{mydef}

In eqn(\ref{eq:fax-single}), $x$ is a 1-by-1 matrix and the result
$\pdiff{f}{x}=a$ is also a 1-by-1 matrix. In eqn(\ref{eq:fax-multi}), 
$x$ is a column vector(known as n-by-1 matrix) and the result
$\pdiff{f}{x}=a$ has the same size. 

\begin{myex}
\label{ex:tran_x}
By this definition, we have:
\begin{equation}
	\pdiff{f}{\tran{x}} = \tran{(\pdiff{f}{x})} = \tran{a}
\end{equation}
Note that we only use the organization definition in this example. 
Later we'll show that with some matrix properties, this formula 
can be derived without using $\pdiff{f}{x}$ as a bridge. 
\end{myex}

\subsection{Deal with Inner Product}

\begin{mythm}
	\label{thm:inner_product}
	If there's a multivariate scalar function $f(x) = \tran{a}x$, 
	we have $\pdiff{f}{x} = a$.
\end{mythm}

\begin{proof}
See introductary example. 
\end{proof}

Since $\tran{a}x$ is scalar, we can write it equivalently as the 
trace of its own. Thus, 
\begin{myprop}
	\label{prop:trace_inner_product}
	If there's a multivariate scalar function $f(x) = \tr{\tran{a}x}$, 
	we have $\pdiff{f}{x} = a$.
\end{myprop}

$\tr{\bullet}$ is the operator to sum up diagonal elements of a matrix. 
In the next section, we'll explore more properties of trace. 
As long as we can transform our target function into the form of 
theorem(\ref{thm:inner_product}) or proposition(\ref{prop:trace_inner_product}), 
the result can be written out directly. Notice in 
proposition(\ref{prop:trace_inner_product}), $a$ and $x$ are both vectors. 
We'll show later as long as their sizes agree, it holds for matrix $a$ and $x$. 

\subsection{Properties of Trace}

\begin{mydef}
Trace of square matrix is defined as: $\tr{A} = \sum_{i}{A_{ii}}$
\end{mydef}

\begin{mythm}
\label{thm:trace_prop}
Matrix trace has the following properties:
\begin{itemize}
	\item (1) $\tr{A+B} = \tr{A} + \tr{B}$
	\item (2) $\tr{cA} = c \tr{A}$
	\item (3) $\tr{AB} = \tr{BA}$
	\item (4) $\tr{A_1A_2 \ldots A_n}=\tr{A_nA_1 \ldots A_{n-1}}$
	\item (5) $\tr{\tran{A}B} = \sum_{i}\sum_{j}{A_{ij}B_{ij}}$
	\item (6) $\tr{A} = \tr{\tran{A}}$
\end{itemize}
where $A,B$ are matrices with proper sizes, and $c$ is a scalar value. 
\end{mythm}

\begin{proof}
See wikipedia \cite{wiki_trace} for the proof. 
\end{proof}

Here we explain the intuitions behind each property
to make it easier to remenber. Property(1)
and property(2) shows the linearity of trace. 
Property(3) means two matrices' multiplication inside a 
the trace operator is commutative. Note that the matrix 
multiplication without trace is not commutative and 
the commutative property inside the trace does not hold 
for more than 2 matrices. Property (4) is the proposition of 
property (3) by considering $A_1A_2 \ldots A_{n-1}$ 
as a whole. It is known as cyclic property, so that you can 
rotate the matrices inside a trace operator. Property (5) 
shows a way to express the sum of element by element product using 
matrix product and trace. Note that inner product of two vectors 
is also the sum of element by element product. Property (5) 
resembles the vector inner product by form($\tran{A}B$). 
The author regards property (5) as the extension of inner product 
to matrices(Generalized Inner Product). 

\subsection{Deal with Generalized Inner Product}

\begin{mythm}
	\label{thm:gip}
	If there's a multivariate scalar function $f(x) = \tr{\tran{A}x}$, 
	we have $\pdiff{f}{x} = A$. ($A,x$ can be matrices). 
\end{mythm}

\begin{proof}
	Using property (5) of trace, we can write $f$ as:
	\begin{equation}
		f(x) = \tr{\tran{A}x} = \sum_{ij}A_{ij}x_{ij}
	\end{equation}
	It's easy to show:
	\begin{equation}
		\pdiff{f}{x_{ij}} = \pdiff{(\sum_{ij}A_{ij}x_{ij})}{x_{ij}} = A_{ij}
	\end{equation}
	Organize elements using definition(\ref{def:org}), it is proved. 
\end{proof}

With this theorem and properties of trace we revisit example(\ref{ex:tran_x}). 
\begin{myex}
	For vector $a,x$ and function $f(x) = \tran{a}x$
	\begin{eqnarray}
	 &&\pdiff{f}{\tran{x}} \\
		&=& \pdiff{( \tran{a}x )}{\tran{x}} \\
	\text{($f$ is scalar)}	&=& \pdiff{( \tr{\tran{a}x} )}{\tran{x}} \\
	\text{(property(3))}	&=& \pdiff{( \tr{x\tran{a}} )}{\tran{x}} \\
	\text{(property(6))}	&=& \pdiff{( \tr{a\tran{x}} )}{\tran{x}} \\
	\text{(property of transpose)}	&=& \pdiff{( \tr{\tran{(\tran{a})}\tran{x}} )}{\tran{x}} \\
	\text{(theorem(\ref{thm:gip}))}	&=& \tran{a}
	\end{eqnarray}
	The result is the same with example(\ref{ex:tran_x}), 
	where we used the basic definition. 
\end{myex}

The above example actually demonstrates the usual way of handling a 
matrix derivative problem. 

\subsection{Define Matrix Differential}

Although we want matrix derivative at most time, it turns out 
matrix differential is easier to operate due to the 
form invariance property of differential. Matrix differential 
inherit this property as a natural consequence of the 
following definition. 

\begin{mydef}
	\label{def:mdiff}
	Define matrix differential:
	\begin{equation}
		\dif A = \left[
		\begin{matrix}
			\dif A_{11} & \dif A_{12} & \ldots &\dif A_{1n}\\
			\dif A_{21} & \dif A_{22} & \ldots &\dif A_{2n}\\
			\vdots & \vdots & \ddots & \vdots \\
			\dif A_{m1} & \dif A_{m2} & \ldots &\dif A_{mn}\\
		\end{matrix}
		\right]
	\end{equation}	 
\end{mydef}

\begin{mythm}
Differential operator is distributive through trace operator:
$\dif \tr{A} = \tr{\dif A}$
\end{mythm}

\begin{proof}
	\begin{eqnarray}
		\text{LHS} &=& \dif (\sum_{i}{A_{ii}}) = \sum_{i}{\dif A_{ii}}\\
		\text{RHS} &=&  \tr{
			\begin{matrix}
			\dif A_{11} & \dif A_{12} & \ldots &\dif A_{1n}\\
			\dif A_{21} & \dif A_{22} & \ldots &\dif A_{2n}\\
			\vdots & \vdots & \ddots & \vdots \\
			\dif A_{m1} & \dif A_{m2} & \ldots &\dif A_{mn}\\
			\end{matrix}
		} \\
		&=& \sum_{i}{\dif A_{ii}} = \text{LHS} 
	\end{eqnarray}
\end{proof}

Now that matrix differential is well defined, we want to relate it 
back to matrix derivative. The scalar version differential and 
derivative can be related as follows:
\begin{equation}
	\dif f = \pdiff{f}{x} \dif x
	\label{eq:scalar_dif_der}
\end{equation}
So far, we're dealing with scalar function $f$ and matrix variable $x$. 
$\pdiff{f}{x}$ and $\dif x$ are both matrix according to definition.
In order to make the quantities in eqn(\ref{eq:scalar_dif_der}) equal, 
we must figure out a way to make the RHS a scalar. It's not surprising
that trace is what we want. 

\begin{mythm}
	\label{thm:matrix_dif_der}
	\begin{equation}
		\dif f = \tr{\tran{(\pdiff{f}{x})} \dif x}
		\label{eq:matrix_dif_der}
	\end{equation}
	for scalar function $f$ and arbitrarily sized $x$. 
\end{mythm}

\begin{proof}
	\begin{eqnarray}
		\text{LHS} &=& \dif f \\
		\text{(definition of scalar differential)}
		&=& \sum_{ij}{\pdiff{f}{x_{ij}}\dif x_{ij}} \\
		\text{RHS} &=&  \tr{\tran{(\pdiff{f}{x})} \dif x} \\
		\text{(trace property (5))}&=& \sum_{ij}(\pdiff{f}{x})_{ij} (\dif x)_{ij} \\
		\text{(definition(\ref{def:mdiff}))} &=& \sum_{ij}(\pdiff{f}{x})_{ij} \dif x_{ij} \\
		\text{(definition(\ref{def:org}))} &=& \sum_{ij}\pdiff{f}{x_{ij}} \dif x_{ij} \\
		&=& \text{LHS}
	\end{eqnarray}
\end{proof}

Theorem(\ref{thm:matrix_dif_der}) is the bridge between matrix 
derivative and matrix differential. We'll see in later applications 
that matrix differential is more convenient to manipulate. 
After certain manipulation we can get the form of theorem(\ref{thm:matrix_dif_der}). 
Then we can directly write out matrix derivative using this theorem. 

\subsection{Matrix Differential Properties}

\begin{mythm}
	\label{thm:mdiff_prop}
	We claim the following properties of matrix differential:
	\begin{itemize}
		\item $\dif (cA) = c \dif A$
		\item $\dif (A+B) = \dif A + \dif B$
		\item $\dif (AB) = \dif A B + A \dif B$
	\end{itemize}
\end{mythm}

\begin{proof}
	They're all natural consequences given the definition(\ref{def:mdiff}). 
	We only show the 3rd one in this document. Note that the equivalence 
	holds if LHS and RHS are equivalent element by element. We consider 
	the (ij)-th element. 
	\begin{eqnarray}
		\text{LHS}_{ij} &=& \dif (\sum_{k}{A_{ik}B_{kj}}) \\
		&=& \sum_{k}{(\dif A_{ik}B_{kj} + A_{ik} \dif B_{kj})} \\
		\text{RHS}_{ij} &=& (\dif A B)_{ij} + (A \dif B)_{ij} \\
		&=& \sum_{k}{\dif A_{ik}B_{kj}} + \sum_{k}{A_{ik}\dif B_{kj}} \\
		&=& \text{LHS}_{ij}
	\end{eqnarray}
\end{proof}

\begin{myex}
	Given the function $f(x) = \tran{x} A x$, where 
	$A$ is square and $x$ is a column vector, we can 
	compute:
	\begin{eqnarray}
		\dif f &=& \dif \tr{ \tran{x} A x } \\
		&=& \tr{\dif(\tran{x} A x)} \\
		&=& \tr{\dif(\tran{x}) A x + \tran{x} \dif(A x)} \\
		&=& \tr{\dif(\tran{x}) A x + \tran{x} \dif A x + \tran{x} A \dif x} \\
		\text{(A is constant)}&=& \tr{\dif \tran{x} A x + \tran{x} A \dif x} \\
		&=& \tr{\dif \tran{x} A x} + \tr{\tran{x} A \dif x} \\
		&=& \tr{\tran{x} \tran{A} \dif x} + \tr{\tran{x} A \dif x} \\
		&=& \tr{\tran{x} \tran{A} \dif x + \tran{x} A \dif x} \\
		&=& \tr{(\tran{x} \tran{A} + \tran{x} A) \dif x}
	\end{eqnarray}
	Using theorem(\ref{thm:matrix_dif_der}), we obtain the derivative:
	\begin{equation}
		\pdiff{f}{x} = \tran{ (\tran{x} \tran{A} + \tran{x} A) }
		= Ax + \tran{A} x
	\end{equation}
	When $A$ is symmetric, it simplifies to:
	\begin{equation}
		\pdiff{f}{x} = 2Ax
	\end{equation}
	Let $A=I$, we have:
	\begin{equation}
		\pdiff{(\tran{x}x)}{x} = 2x
	\end{equation}
\end{myex}

\begin{myex}
	For a non-singular square matrix $X$, we have $XX^{-1} = I$. 
	Take matrix differentials at both sides:
	\begin{equation}
		0=\dif I = \dif (XX^{-1}) = \dif X X^{-1} + X \dif(X^{-1})
	\end{equation}
	Rearrange terms:
	\begin{equation}
		\dif(X^{-1})=-X^{-1} \dif X X^{-1}  
\end{equation}	 
\end{myex}

\subsection{Schema of Hanlding Scalar Function}

The above example already demonstrates the general schema. 
Here we conclude the process:
\begin{enumerate}
	\item $\dif f = \dif \tr{f} =\tr{\dif f} $
	\item Apply trace properties(see theorem(\ref{thm:trace_prop}))
	and matrix differential properties(see theorem(\ref{thm:mdiff_prop}))
	to get the following form:
		\begin{equation}
			\dif f = \tr{\tran{A}x}
		\end{equation}
	\item Apply theorem(\ref{thm:matrix_dif_der}) to get:
		\begin{equation}
			\pdiff{f}{x} = A
		\end{equation}
\end{enumerate}

To this point, you can handle many problems. In this schema, 
matrix differential and trace play crucial roles. Later we'll 
deduce some widely used formula to facilitate potential 
applications. As you will see, although we rely on matrix differential 
in the schema, the deduction of certain formula may be more easily done 
using matrix derivatives. 

\subsection{Determinant}

For a background of determinant, please refer to \cite{wiki_det}. 
We first quote some definitions and properties without proof:
\begin{mythm}
	Let $A$ be a square matrix:
	\begin{itemize}
		\item The minor $M_{ij}$ is obtained by remove 
		i-th row and j-th column of $A$ and then take determinant
		of the resulting (n-1) by (n-1) matrix. 
		\item The ij-th cofactor is defined as $C_{ij} = (-1)^{i+j}M_{ij}$. 
		\item If we expand determinant with respect to the i-th row, 
		$\det(A) = \sum_{j}{A_{ij}C_{ij}}$. 
		\item The adjugate of $A$ is defined as 
		$\adj(A)_{ij}=(-1)^{i+j}M_{ji}=C_{ji}$. So 
		$\adj(A) = \tran{C}$
		\item For non-singular matrix $A$, we have:
		$A^{-1} = \frac{\adj(A)}{\det(A)} = \frac{\tran{C}}{\det(A)}$
	\end{itemize}
\end{mythm}

Now we're ready to show the derivative of determinant. Note that determinant
is just a scalar function, so all techniques discussed above is applicable. 
We first write the derivative element by element. Expanding determinant
on the i-th row, we have:
\begin{equation}
	\pdiff{\det(A)}{A_{ij}} = \pdiff{(\sum_{j}{A_{ij}C_{ij}})}{A_{ij}} = C_{ij}
\end{equation}
First equality is from determinant definition and second equality is 
by the observation that only coefficient of $A_{ij}$ is left. Grouping all 
elements using definition(\ref{def:org}), we have:
\begin{equation}
	\pdiff{\det(A)}{A} = C = \tran{\adj(A)}
\end{equation}
If $A$ is non-singular, we have:
\begin{equation}
	\pdiff{\det(A)}{A} = \tran{(\det(A)A^{-1})} =\det(A)\tran{(A^{-1})} 
\end{equation}

Next, we use theorem(\ref{thm:matrix_dif_der}) to give the differential 
relationship:
\begin{eqnarray}
	\dif \det(A) &=& \tr{ \tran{(\pdiff{\det(A)}{A})} \dif A} \\
	&=& \tr{ \tran{( \det(A)\tran{(A^{-1})}  )} \dif A} \\
	&=& \tr{ \det(A)A^{-1}  \dif A}
\end{eqnarray}

In many practical problem, the log determinant is more widely used:
\begin{eqnarray}
	\pdiff{\ln \det(A)}{A} = \frac{1}{\det(A)} \pdiff{\det(A)}{A} = \tran{(A^{-1})}
\end{eqnarray}
The first equality comes from chain rule of ordinary calculus(
$\ln \det(A)$ and $\det(A)$ are both scalars). Similarly, we derive
for differential:
\begin{equation}
	\dif \ln \det(A) = \tr{ A^{-1}  \dif A}
\end{equation}


\section{Application}
\label{sec:application}

\subsection{The 2nd Induced Norm of Matrix}

The induced norm of matrix is defined as \cite{wiki_norm}:
\begin{equation}
	||A||_p = \max_{x}{\frac{||Ax||_p}{||x||_p}}
\end{equation}
where $||\bullet||_p$ denotes the p-norm of vectors. Now we solve 
for $p=2$. (By default, $||\bullet||$ means $||\bullet||_2$)

The problem can be restated as:
\begin{equation}
	||A||^2 = \max_{x}{\frac{||Ax||^2}{||x||^2}}
\end{equation}
since all quantities involved are non-negative. Then we consider a 
scaling of vector $x' = tx$, thus:
\begin{equation}
	||A||^2 = \max_{x'}{\frac{||Ax'||^2}{||x'||^2}} 
	=\max_{x}{\frac{||tAx||^2}{||tx||^2}}
	=\max_{x}{\frac{t^2||Ax||^2}{t^2||x||^2}}
	=\max_{x}{\frac{||Ax||^2}{||x||^2}}
\end{equation}
This shows the invariance under scaling. Now we can restrict our attention 
to those $x$ with $||x||=1$, and reach the following formulation:
\begin{eqnarray}
	\maximize && f(x) = ||Ax||^2 \\
	s.t. && ||x||^2 = 1
\end{eqnarray}

The standard way to handle this constrained optimization is 
using Lagrange relaxation:
\begin{equation}
	L(x) = f(x) - \lambda (||x||^2 - 1)
\end{equation}
Then we apply the general schema of handling scalar function 
on $L(x)$. First take differential:
\begin{eqnarray}
	\dif L(x) &=& \dif \tr{L(x)} \\
	&=& \tr{ \dif( L(x) ) } \\
	&=& \tr{ \dif( \tran{x}\tran{A}Ax - \lambda(\tran{x}x - 1)) }  \\
	&=& \tr{ 2 \tran{x} \tran{A}A \dif x - \lambda (2 \tran{x} \dif x) } \\
	&=& \tr{ (2 \tran{x} \tran{A}A - 2\lambda \tran{x}) \dif x }
\end{eqnarray}
Next write out derivative:
\begin{equation}
	\pdiff{L}{x} = 2 \tran{A}A x - 2\lambda x
\end{equation}
Let $\pdiff{L}{x} = 0$, we have:
\begin{equation}
	(\tran{A}A) x = \lambda x
\end{equation}
That means x is the eigen vector of $(\tran{A}A)$
(normalized to $||x||=1$), and $\lambda$ 
is corresponding eigen value. We plug this result back to objective 
function:
\begin{equation}
	f(x) = \tran{x}(\tran{A}Ax) =\tran{x} (\lambda x) = \lambda
\end{equation}
which means, to maximize $f(x)$, we should pick the maximum 
eigen value:
\begin{equation}
	||A||^2 = \max f(x) = \lambda_{\max}(\tran{A}A)
\end{equation}
That is:
\begin{equation}
	||A|| = \sqrt{\lambda_{\max}(\tran{A}A)} = \sigma_{\max}(A)
\end{equation}
where $\sigma_{\max}$ denotes the maximum singular value. 
If $A$ is real symmetric, $\sigma_{\max}(A) = \lambda_{\max}(A) $. 

Now we consider a real symmetric $A$ and check whether:
\begin{equation}
	\lambda^2_{\max}(A) = \max_{x}{\frac{||Ax||^2}{||x||^2}}
	= \max_{x}{\frac{\tran{x}\tran{A}Ax}{\tran{x}x}}
\end{equation}

\begin{proof}
Since $\tran{A}A$ is real symmetric, it has an orthnormal basis 
formed by $n$ eigen vectors, $v_1, v_2, \dots, v_n$, with 
eigen values $\lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_n$. We can 
write $x=\sum_{i}{c_iv_i}$, where $c_i=<x,v_i>$. Then, 
\begin{eqnarray}
	&&\frac{\tran{x}\tran{A}Ax}{\tran{x}x} \\
	\text{($v_k$ is an orthnormal set)}&=& \frac{\sum_i \lambda_i c_i^2}{\sum_{i}{c_i^2}} \\
	&\le & \frac{\sum_i \lambda_1 c_i^2}{\sum_{i}{c_i^2}} \\
	&=& \lambda_1
\end{eqnarray}
Now we have proved an upper bound for $||A||^2$. We show this bound is achievable 
by assigning $x=v_1$. 
\end{proof}
	

\section{Cheat Sheet}
\label{sec:cheat}


%>============================================
\section*{Acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}
Thanks prof. XU, Lei's tutorial on matrix calculus. 
Besides, the author also benefit a lot from other online 
materials. 
%<=======Acknowledgements ENd=================

%>============================================
\addcontentsline{toc}{section}{References}
\input{../reference/gen_bib.bbl}
%<=======Bibliography ENd=====================

%>============================================
\section*{Appendix}
\addcontentsline{toc}{section}{Appendix}

%<=======Appendix ENd=========================

\end{document}
